===============
MOM6 GPU Update
===============

:author: Marshall Ward
:organization: NOAA-GFDL
:description: Update of MOM6 GPU activities
:date: 2025-07-08


Collaborators
=============

* GFDL

   - **Marshall Ward**
   - Alistair Adcroft
   - Robert Hallberg

* ACCESS-NRI (Australia)

   - **Ed Yang**
   - Jorge Galvez Vallejo
   - Micael Oliveira


Frameworks
==========

OpenACC
   Intially successful, but NVIDIA-specific.

OpenMP
   More platform-independent, mostly equivalent

do concurrent
   Language-intrinsic, best performnace (so far)


Implementation
==============

Explicit kernels were ok but less performant

.. code:: fortran

   !$omp target
   do k=1,nz
     !$omp parallel loop collapse(2)
     do j=js,je ; do I=Isq,Ieq
       u_bc_accel(I,j,k) = (CS%CAu_pred(I,j,k) + CS%PFu(I,j,k)) + CS%diffu(I,j,k)
     enddo ; enddo
   enddo
   !$omp end target

``do concurrent`` was simpler, and faster:

.. code:: fortran

   do k=1,nz
     do concurrent(I=Isq:Ieq, j=js:je)
       u_bc_accel(I,j,k) = (CS%CAu_pred(I,j,k) + CS%PFu(I,j,k)) + CS%diffu(I,j,k)
     enddo
   enddo


Hybrid Approach
===============

Move data with OpenMP, kernels with ``do concurrent``

.. code:: fortran

   !$omp target enter data map(to: G, G%mask2dT, G%areaT)
   !$omp target enter data map(alloc: Area_h)

   do concurrent (I=Isq-1:Ieq+2, j=Jsq-1:Jeq+2)
     Area_h(i,j) = G%mask2dT(i,j) * G%areaT(i,j)
   enddo

   !$omp target exit data map(from: Area_h)

.. Runtime dropped by 1/3: 29s -> 23s


Managed memory?
---------------

``do concurrent`` is conservative: it tends to copy, even if the data is
already on the GPU!

"Managed" memory was slow (30s -> 180s), so we are doing *manual management*


Experiments
===========

* ``double_gyre``

   - Idealized adiabatic layers

* ``benchmark``

   - Thermodynamics

* ...?


Development
===========

Source repo: ``dev/gpu``
   https://github.com/marshallward/MOM6/tree/dev/gpu 

Design document
   https://github.com/marshallward/mom6-gpu-report/


Dyncore Status
==============

======================= =========================
Continuity_             **(Testing)**
Barotropic_             **Finished** (\*)
`Vertical Viscosity`_   kji refactor complete
`Horizontal Viscosity`_ **Finished**
`Coriolis/MomFlux`_     **Finished**
`Pressure Force`_       **Finished**
======================= =========================

Bitwise-equivalent answers to CPU

.. _Continuity: https://github.com/edoyango/MOM6/blob/elemental-zonal_flux_layer/src/core/MOM_continuity_PPM.F90
.. _Barotropic: https://github.com/marshallward/MOM6/blob/dev/gpu/src/core/MOM_barotropic.F90
.. _Vertical Viscosity: https://github.com/marshallward/MOM6/blob/dev/gpu/src/parameterizations/vertical/MOM_vert_friction.F90
.. _Horizontal Viscosity: https://github.com/marshallward/MOM6/blob/dev/gpu/src/parameterizations/lateral/MOM_hor_visc.F90
.. _Coriolis/MomFlux: https://github.com/marshallward/MOM6/blob/dev/gpu/src/core/MOM_CoriolisAdv.F90
.. _Pressure Force: https://github.com/marshallward/MOM6/blob/dev/gpu/src/core/MOM_PressureForce_FV.F90


Scaling (so far)
================

.. image:: img/gpu_scaling.png


Optimized
---------

.. image:: img/gpu_scaling_opt.png


GPU vs CPU
==========

.. image:: img/gpu_vs_cpu.png

(1 â†’ 4 â†’ 16 cores)


Resources
=========

* NVIDIA

   * A100
   * H100
   * Grace Hopper (?)

* AMD

   * MI300


A100 vs H100
============

.. image:: img/a100_vs_h100.png


H100 vs CPU
===========

.. image:: img/h100_vs_cpu.png


H100 opt vs CPU
---------------

.. image:: img/h100_o2_vs_cpu_o0.png

(CPU with ``-O2`` has runtime errors... ðŸ¤·)


Current Goals
=============

* Proficiency with OpenMP and ``do concurrent``

* Majority of dynamic core ported to GPU
  
* Good scaling so far?

  - CPU-competitive? ðŸ˜¶


Next Steps
==========

* Complete ``double_gyre``

  * Finish continuity and vertical viscosity

  * Work through dyncore loop and misc

  * Resolve any scaling issues

* Repeat for ``benchmark``

  * Equation of state (T, S, ...)

* Investigate GPU performance limits?


Long term
=========

* Realistic configurations (OM5?)

* Extreme scaling (multinode GPU)

* Synchronization with TURBO effort

